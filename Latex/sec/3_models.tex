\section{Models Used}

%-------------------------------------------------------------------------
\subsection{Models and Parameters}
After the data engineering phase, several machine learning models were trained and optimized to predict the outcome of each PokÃ©mon battle. 
Hyperparameter tuning was conducted through a combination of grid and randomized search strategies, using validation accuracy as the main performance metric. 
The best configurations and their corresponding accuracies are summarized in Table~\ref{tab:best_params}.

\begin{table}[h!]
\centering
\caption{Best parameters and accuracy for each model.}
\label{tab:best_params}
\begin{adjustbox}{max width=\columnwidth}
\small
\begin{tabular}{|l|>{\raggedright\arraybackslash}p{5cm}|c|}
\hline
\textbf{Model} & \textbf{Best Parameters} & \textbf{Accuracy} \\
\hline
KNN & n\_neighbors=28, weights=distance, metric=minkowski (p=2) & 0.8193 \\
\hline
Logistic Regression & solver=lbfgs, penalty=l2, max\_iter=10000, Cs=10, cv=5, n\_jobs=8 & \textbf{0.8405} \\
\hline
Decision Tree & criterion=gini, max\_depth=6, splitter=best & 0.8152 \\
\hline
Random Forest & criterion=entropy, n\_estimators=100, max\_depth=97, max\_features=sqrt & 0.8304 \\
\hline
XGBoost & objective=binary:logistic, max\_depth=6, reg\_lambda=0.47 & 0.8290 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table}

%-------------------------------------------------------------------------
\subsection{Ensemble}
Eventually, both an ensemble model and a stacking meta-model were implemented using the same five base learners (Logistic Regression, K-Nearest Neighbors, Decision Tree, Random Forest, and XGBoost). The ensemble relied on majority voting among the individual predictions, while the stacking model combined them through a Logistic Regression meta-learner, achieving a more stable and robust performance.

